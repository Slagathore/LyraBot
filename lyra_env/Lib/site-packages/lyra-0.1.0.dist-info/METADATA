Metadata-Version: 2.3
Name: lyra
Version: 0.1.0
Summary: A self-improving AI with recursive learning and modular tool integration
Author: Cole
Author-email: charcham7@gmail.com
Requires-Python: >=3.10,<3.11
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Provides-Extra: all
Provides-Extra: local-llm
Provides-Extra: openai
Provides-Extra: rasa
Provides-Extra: video
Requires-Dist: accelerate (>=0.25.0) ; extra == "local-llm" or extra == "all"
Requires-Dist: bitsandbytes (>=0.41.0) ; extra == "local-llm" or extra == "all"
Requires-Dist: diffusers (>=0.25.0) ; extra == "video" or extra == "all"
Requires-Dist: faiss-cpu (>=1.7.0)
Requires-Dist: langchain (>=0.0.267) ; extra == "openai" or extra == "all"
Requires-Dist: langchain-openai (>=0.0.1) ; extra == "openai" or extra == "all"
Requires-Dist: llama-cpp-python (>=0.2.10) ; extra == "local-llm" or extra == "all"
Requires-Dist: numpy
Requires-Dist: openai (>=1.0.0) ; extra == "openai" or extra == "all"
Requires-Dist: python-dotenv
Requires-Dist: rasa[full] (>=3.6.21,<4.0.0) ; extra == "rasa" or extra == "all"
Requires-Dist: requests (>=2.28.0)
Requires-Dist: sentence-transformers (>=2.2.2)
Requires-Dist: sqlalchemy (<2.0.0)
Requires-Dist: sqlalchemy-utils
Requires-Dist: textblob (>=0.17.1)
Requires-Dist: torch (>=2.1.0) ; extra == "local-llm" or extra == "video" or extra == "all"
Requires-Dist: tqdm
Requires-Dist: transformers (>=4.36.0) ; extra == "local-llm" or extra == "all"
Description-Content-Type: text/markdown

# Lyra
A conversational AI with long-term + emotional + contextually sensitive memory, multi-modal capabilities, many different skills to interact + help the user, curious thought, ability to learn modules on the go, and finally with full self driven self-improvement through recursive feedback looping + AI buddy system.

## Features
- **Memory**: FAISS for long-term memory, emotional + contextual understanding
- **LLM**: Support for both OpenAI API and local LLM models
- **Personality**: Dynamic mood and OCEAN traits with self-improvement loop
- **Multi-Modal**: Text and video generation capabilities

## New in this version
- **Local LLM Support**: Run with Qwen2.5-QwQ-37B-Eureka or other local models
- **Video Generation**: Create videos from text descriptions using Wan2.1-I2V
- **Model Manager**: Download, manage and test models with a simple CLI
- **Improved Dependencies**: Fixed dependency conflicts between components

## Quick Start

### Method 1: Simple Installation (Recommended for Windows)
```bash
# Run the simple installation script
.\simple_install.bat

# Activate the environment
.\lyra_env\Scripts\activate

# Run Lyra
python -m lyra.main
```

### Method 2: Using Virtual Environment (Recommended)
```bash
# Start Lyra (setup + run)
python start_lyra.py

# Or set up environment separately
python resolve_dependencies.py --langchain

# Then activate environment and run
# On Windows:
.\lyra_env\Scripts\activate
# On macOS/Linux:
source lyra_env/bin/activate

# Run Lyra
python -m lyra.main
```

### Method 3: Using the Model Manager
```bash
# List available models
python model_manager.py list

# Download models (may take time)
python model_manager.py download --type text
python model_manager.py download --type video

# Set active models
python model_manager.py set --type text --model-path "path/to/model.gguf"

# Test models
python model_manager.py test --type text --prompt "Hello Lyra!"
```

### Method 4: Using Poetry (Advanced)
```bash
# Install using Poetry with local LLM dependencies
poetry install --extras "local-llm"

# For video generation
poetry install --extras "video"

# Run using Poetry
poetry run python -m lyra.main
```

## Troubleshooting

### Installation Issues
If you encounter issues during installation:

1. Try the simple installation method (`simple_install.bat`) which installs core dependencies directly
2. If that fails, try a clean reinstall: `clean_reinstall.bat`
3. On Windows, make sure you're using Command Prompt (not PowerShell) for running batch files
4. Check that Python 3.10+ is installed and in your PATH

For more detailed help, see the [installation guide](docs/HOWTO_LOCAL_LLM.md).

## Environment Variables
Create a file named `.env` in the project root directory with settings:
```
OPENAI_API_KEY=your_api_key_here
```

## Development
To install development tools:
```bash
python resolve_dependencies.py --dev
```


